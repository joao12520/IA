{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "527ca0fc",
   "metadata": {},
   "source": [
    "# IA - Projeto 2: Fraude Bancária"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5edd281",
   "metadata": {},
   "source": [
    "Para este Projeto foi-nos proposta a análise de modelos de IA treinados por 3 algoritmos de supervised learning à nossa escolha de forma a percebermos como funcionam e o pré-processamento que a análise de dados e de algoritmos de classsificação precisam.\n",
    "\n",
    "Assim, para o desenvolvimento deste trabalho selecionamos treinar o nosso modelo com os seguintes algoritmos de supervised learning no âmbito dos problemas de classsificação:\n",
    "- Decision Tree\n",
    "- Neural Network\n",
    "- K Nearest Neighbour (KNN)\n",
    "\n",
    "Ao longo deste Notebook vamos apresentar todos os passos que demos para chegar aos resultados obtidos pelos nossos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a657f9",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Este dataset que estamos a usar para treinar o nosso algoritmo contém 37 variáveis e 10000 linhas de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e041f5e6",
   "metadata": {},
   "source": [
    "## Data pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0aa191",
   "metadata": {},
   "source": [
    "Depois de analisar o dataset verificamos que para treinarmos os modelos, seria necessário fazer um pré-processamento dos dados para os limpar, balancear (uma vez que são desbalanceados, o número de fraudes é mesmo inferior ao número de não fraudes), converter para a mesma escala e transformar os valores das varíaveis categóricas por valores numéricos.\n",
    "Assim, apresentamos em seguida os passos que demos durante esta fase do Projeto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0345e35",
   "metadata": {},
   "source": [
    "### Deal with missing values\n",
    "\n",
    "Corrijimos os valores em falta, usando este código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c9d0adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_missing_values(self):\n",
    "    columns_to_check = ['prev_address_months_count', 'current_address_months_count', 'intended_balcon_amount',\n",
    "                        'bank_months_count', 'session_length_in_minutes']\n",
    "    self.data = self.data[~(self.data[columns_to_check] == -1).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807eb45b",
   "metadata": {},
   "source": [
    "### Normalize values from variables\n",
    "\n",
    "Foi necessário também fazer conversão de valores, uma vez que tínhamos variáveis categóricas que precisamos de transformar em valores numéricos de forma a podermos usar os algoritmos da library sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae4ce26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(self):\n",
    "    numeric_cols = ['prev_address_months_count', 'current_address_months_count', 'customer_age',\n",
    "                    'days_since_request', 'intended_balcon_amount', 'zip_count_4w', 'velocity_6h', 'velocity_24h',\n",
    "                    'velocity_4w', 'bank_branch_count_8w', 'date_of_birth_distinct_emails_4w', 'credit_risk_score',\n",
    "                    'bank_months_count', 'proposed_credit_limit', 'session_length_in_minutes',\n",
    "                    'device_distinct_emails_8w', 'month']\n",
    "    string_cols = ['payment_type', 'employment_status', 'housing_status', 'source', 'device_os']\n",
    "    for strCol in string_cols:\n",
    "        self.encoder.fit(self.data[strCol])  # Fit the encoder to the data\n",
    "        self.data[strCol] = self.encoder.transform(self.data[strCol])  # Encode and replace the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585fff1",
   "metadata": {},
   "source": [
    "Usamos também uma função de transformação de determinados parâmetros tendo em conta os dados de treino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "265a8df2",
   "metadata": {},
   "outputs": [],
   "source": [
    " def scale_data(self):\n",
    "        self.data = pd.DataFrame(self.scaler.fit_transform(self.data), columns=self.data.columns)\n",
    "        print(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a6877f",
   "metadata": {},
   "source": [
    "## Algorithms used to train models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa49639",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "Começamos por apresentar os resultados obtidos com o Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f3df88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeFraudDetector:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.xtrain, self.xtest, self.ytrain, self.ytest = self._split_data()\n",
    "\n",
    "    def _split_data(self):\n",
    "        x = self.data.drop('fraud_bool', axis=1)\n",
    "        y = self.data['fraud_bool']\n",
    "        X_encoded = pd.get_dummies(x, columns=['payment_type', 'employment_status', 'housing_status', 'source',\n",
    "                                               'device_os'])\n",
    "\n",
    "        oversampler = SMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = oversampler.fit_resample(X_encoded, y)\n",
    "\n",
    "        xtrain, xtest, ytrain, ytest = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "        return xtrain, xtest, ytrain, ytest\n",
    "\n",
    "    def train(self):\n",
    "        self.clf = DecisionTreeClassifier(max_depth=5, min_samples_split=5)\n",
    "        self.clf.fit(self.xtrain, self.ytrain)\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        return self.clf.predict(input_data)\n",
    "\n",
    "    def evaluate(self):\n",
    "        y_pred_proba = self.clf.predict_proba(self.xtest)[:, 1]\n",
    "        fpr, tpr, thresholds = roc_curve(self.ytest, y_pred_proba)\n",
    "        auc_roc = roc_auc_score(self.ytest, y_pred_proba)\n",
    "\n",
    "        # Print AUC-ROC score\n",
    "        print(\"AUC-ROC Score:\", auc_roc)\n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            if fpr[i] <= 0.05:\n",
    "                print(\"Threshold: {:.2f}, FPR: {:.2f}, TPR: {:.2f}\".format(threshold, fpr[i], tpr[i]))\n",
    "\n",
    "        # Plot ROC curve\n",
    "        plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc_roc)\n",
    "        plt.plot([0, 1], [0, 1], 'k--')  # Random guessing line\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC)')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588d25dc",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8180bc",
   "metadata": {},
   "source": [
    "Neste modelo usamos o Random Forest para verificarmos se obtíamos melhores resultados do que com a decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65270102",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self._split_data()\n",
    "\n",
    "    def _split_data(self):\n",
    "        X = self.data.drop('fraud_bool', axis=1)\n",
    "        y = self.data['fraud_bool']\n",
    "\n",
    "        # Label encoding\n",
    "        label_encoder = LabelEncoder()\n",
    "        X_encoded = X.copy()\n",
    "        for col in ['payment_type', 'employment_status', 'housing_status', 'source', 'device_os']:\n",
    "            X_encoded[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "        # Sparse one-hot encoding\n",
    "        X_encoded = pd.get_dummies(X_encoded, columns=['payment_type', 'employment_status', 'housing_status', 'source', \n",
    "                                                       'device_os'], sparse=True)\n",
    "\n",
    "        # Perform oversampling using SMOTE\n",
    "        oversampler = SMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = oversampler.fit_resample(X_encoded, y)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def train(self):\n",
    "        self.clf = RandomForestClassifier(n_jobs=-1)  # Enable parallel processing\n",
    "        self.clf.fit(self.X_train, self.y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.clf.predict(X_test)\n",
    "\n",
    "    def evaluate(self):\n",
    "        y_pred_proba = self.clf.predict_proba(self.X_test)[:, 1]\n",
    "        fpr, tpr, thresholds = roc_curve(self.y_test, y_pred_proba)\n",
    "        auc_roc = roc_auc_score(self.y_test, y_pred_proba)\n",
    "\n",
    "        # Print AUC-ROC score\n",
    "        print(\"AUC-ROC Score:\", auc_roc)\n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            if fpr[i] <= 0.05:\n",
    "                print(\"Threshold: {:.2f}, FPR: {:.2f}, TPR: {:.2f}\".format(threshold, fpr[i], tpr[i]))\n",
    "\n",
    "        # Plot ROC curve\n",
    "        plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc_roc)\n",
    "        plt.plot([0, 1], [0, 1], 'k--')  # Random guessing line\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC)')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc27abd8",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n",
    "De seguida, treinamos o nosso modelo com as Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a91aaaeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OneHotEncoder, LabelEncoder, StandardScaler\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#import Sampler\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mFraudDetectorNN\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FraudDetectorNN\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m     18\u001b[0m     sampler \u001b[38;5;241m=\u001b[39m Sampler\u001b[38;5;241m.\u001b[39mSampler(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../datasets/Base.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "\n",
    "#import Sampler\n",
    "from src.FraudDetectorNN import FraudDetectorNN\n",
    "\n",
    "\n",
    "def main():\n",
    "    #sampler = Sampler.Sampler('../datasets/Base.csv')\n",
    "    #data = sampler.sample(50000, 0.2)\n",
    "    data = pd.read_csv('../datasets/Base.csv')\n",
    "    # fraudNN = FraudDetectorNN(data)\n",
    "    x = data.drop(['fraud_bool', 'bank_months_count', 'prev_address_months_count'], axis=1)\n",
    "    y = data['fraud_bool']\n",
    "\n",
    "    for column in ['prev_address_months_count', 'current_address_months_count', 'intended_balcon_amount', 'bank_months_count', \n",
    "                   'session_length_in_minutes']:\n",
    "        values = data[data[column] == -1][column]\n",
    "        print(column, \" : \", len(values))\n",
    "        print()\n",
    "\n",
    "    # Preprocessing\n",
    "    numeric_cols = ['prev_address_months_count', 'current_address_months_count', 'customer_age',\n",
    "                    'days_since_request', 'intended_balcon_amount', 'zip_count_4w', 'velocity_6h',\n",
    "                    'velocity_24h', 'velocity_4w', 'bank_branch_count_8w', 'date_of_birth_distinct_emails_4w',\n",
    "                    'credit_risk_score', 'bank_months_count', 'proposed_credit_limit',\n",
    "                    'session_length_in_minutes', 'device_distinct_emails_8w', 'month']\n",
    "    onehot_encoding = ['payment_type', 'employment_status', 'housing_status', 'device_os']\n",
    "    label_encoding = ['source']\n",
    "    columns_to_check = ['prev_address_months_count', 'current_address_months_count', 'intended_balcon_amount',\n",
    "                        'bank_months_count', 'session_length_in_minutes']\n",
    "\n",
    "    onehot_transformer = ColumnTransformer([('encoder', OneHotEncoder(), onehot_encoding)], remainder='passthrough')\n",
    "    labelencoder = LabelEncoder()\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    for strCol in label_encoding:\n",
    "        labelencoder.fit(x[strCol])\n",
    "        x[strCol] = labelencoder.transform(x[strCol])\n",
    "\n",
    "    x = pd.DataFrame(x)\n",
    "    x = pd.DataFrame(onehot_transformer.fit_transform(x))\n",
    "    x = scaler.fit_transform(x)\n",
    "    x = pd.DataFrame(scaler.fit_transform(x))\n",
    "\n",
    "    oversampler = SMOTE(random_state=42)\n",
    "    x, y = oversampler.fit_resample(x, y)\n",
    "\n",
    "    # Splitting data into training and testing sets\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=0.75, random_state=0)\n",
    "\n",
    "    print(\"Length of x: \", len(xTrain))\n",
    "\n",
    "    neural_network = MLPClassifier(random_state=0)\n",
    "\n",
    "    hidden_layers_list = []\n",
    "    for num_layers in range(1, 3):  # Varying number of hidden layers from 1 to 2\n",
    "        for layer_size in itertools.product(range(20, 101, 10), repeat=num_layers):\n",
    "            hidden_layers_list.append(layer_size)\n",
    "\n",
    "    print(\"Length: \", len(hidden_layers_list))\n",
    "    print(hidden_layers_list)\n",
    "\n",
    "    param_distributions = {'hidden_layer_sizes': hidden_layers_list,\n",
    "                           'activation': ['relu', 'logistic'],\n",
    "                           'solver': ['adam'],\n",
    "                           }\n",
    "\n",
    "    grid_search = RandomizedSearchCV(neural_network, param_distributions, cv=3, n_iter=20, scoring=\"roc_auc\", verbose=3, \n",
    "                                     n_jobs=-1)\n",
    "    grid_search.fit(xTrain, yTrain)\n",
    "\n",
    "    print(\"Best Parameters:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print()\n",
    "\n",
    "    best_neural_network = grid_search.best_estimator_\n",
    "    best_neural_network.fit(xTrain, yTrain)\n",
    "\n",
    "    # Predict probabilities for the testing set\n",
    "    y_pred_proba = best_neural_network.predict_proba(xTest)[:, 1]\n",
    "\n",
    "    # Calculate ROC curve and AUC-ROC score\n",
    "    fpr, tpr, thresholds = roc_curve(yTest, y_pred_proba)\n",
    "    auc_roc = roc_auc_score(yTest, y_pred_proba)\n",
    "\n",
    "    # Print AUC-ROC score\n",
    "    print(\"AUC-ROC Score:\", auc_roc)\n",
    "\n",
    "    # Print threshold, FPR, and TPR for fpr <= 0.05\n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        if fpr[i] <= 0.05:\n",
    "            print(\"Threshold: {:.2f}, FPR: {:.2f}, TPR: {:.2f}\".format(threshold, fpr[i], tpr[i]))\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc_roc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Random guessing line\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cff355d",
   "metadata": {},
   "source": [
    "### KNN\n",
    "\n",
    "Por último, utilizamos o KNN para treinar o nosso modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "487f78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNFraudDetector:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.xtrain, self.xtest, self.ytrain, self.ytest = self._split_data()\n",
    "\n",
    "    def _split_data(self):\n",
    "        x = self.data.drop('fraud_bool', axis=1)\n",
    "        y = self.data['fraud_bool']\n",
    "        X_encoded = pd.get_dummies(x, columns=['payment_type', 'employment_status', 'housing_status', 'source',\n",
    "                                               'device_os'])\n",
    "\n",
    "        oversampler = SMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = oversampler.fit_resample(X_encoded, y)\n",
    "\n",
    "        xtrain, xtest, ytrain, ytest = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "        return xtrain, xtest, ytrain, ytest\n",
    "\n",
    "    def train(self):\n",
    "        self.clf = KNeighborsClassifier(n_neighbors=5)\n",
    "        self.clf.fit(self.xtrain, self.ytrain)\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        return self.clf.predict(input_data)\n",
    "\n",
    "    def evaluate(self):\n",
    "        y_pred_proba = self.clf.predict_proba(self.xtest)[:, 1]\n",
    "        fpr, tpr, thresholds = roc_curve(self.ytest, y_pred_proba)\n",
    "        auc_roc = roc_auc_score(self.ytest, y_pred_proba)\n",
    "\n",
    "        # Print AUC-ROC score\n",
    "        print(\"AUC-ROC Score:\", auc_roc)\n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            if fpr[i] <= 0.05:\n",
    "                print(\"Threshold: {:.2f}, FPR: {:.2f}, TPR: {:.2f}\".format(threshold, fpr[i], tpr[i]))\n",
    "\n",
    "        # Plot ROC curve\n",
    "        plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc_roc)\n",
    "        plt.plot([0, 1], [0, 1], 'k--')  # Random guessing line\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC)')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b63614",
   "metadata": {},
   "source": [
    "Com este modelo conseguimos obter um resultado ROC de **91%**. Um dos parâmetros do algoritmo que usamos `KNeighborsClassifier` que tem bastante impacto neste resultado é no `n_neighbors` (que representa o número de vizinhos do nó atual que vamos visitar). Reparamos que quanto maior o número de vizinhos, pior o desempenho deste modelo. Pelo que o resultado ótimo, foi encontrado com `n_neighbors=5`.\n",
    "\n",
    "Durante o período de testes e experiência, reparamos também que a normalização das variáveis tem um grande impacto neste algoritmo, uma vez que quando usamos outra forma de pre-processamento obtivemos um resultado de 60%. Muito possivelmente, porque os relevantes para o modelo não estavam tratados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828a8612",
   "metadata": {},
   "source": [
    "## Result analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559e7903",
   "metadata": {},
   "source": [
    "Para conseguirmos analisar e comparar os 3 modelos que treinamos, usamos várias métricas de entre as quais destacamos:\n",
    "- **AUC-ROC**\n",
    "- **Confusion Matrix**\n",
    "- **Learning curve**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f345520d",
   "metadata": {},
   "source": [
    "## Comparison between Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49f604",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622fac15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
